[LLM]
# Available models: Any DeepInfra-supported model
# Examples: openai/gpt-oss-20b, meta-llama/Llama-3.3-70B-Instruct-Turbo, meta-llama/Llama-3.2-3B-Instruct
model = openai/gpt-oss-20b

# Gating model for intervention decisions (smaller/faster model recommended)
gating_model = meta-llama/Llama-3.2-3B-Instruct

[STT]
# Available providers: speechmatics, deepgram, deepgram-flux
# - speechmatics: Speechmatics with SMART_TURN detection
# - deepgram: Deepgram Nova-2 with endpoint detection
# - deepgram-flux: Deepgram Flux with built-in turn detection (recommended)
provider = speechmatics

[TTS]
# Available providers: elevenlabs, qwen3
provider = qwen3

# Qwen3-TTS Configuration (only used if provider = qwen3)
# Available models: Qwen/Qwen3-TTS-12Hz-0.6B-Base, Qwen/Qwen3-TTS-12Hz-1.7B-Base
qwen3_model = Qwen/Qwen3-TTS-12Hz-0.6B-Base
# Available devices: mps (Mac), cuda (NVIDIA), cpu
qwen3_device = mps
# Reference audio file for voice cloning (relative to project root)
qwen3_ref_audio = assets/audio/tars-clean-compressed.mp3

[Emotional]
# Enable real-time emotional state monitoring via video
enabled = true
# How often to sample video frames (in seconds)
sampling_interval = 3.0
# How many consecutive negative states before intervention
intervention_threshold = 2

[Display]
# Enable TARS Raspberry Pi display integration
enabled = false
# URL of TARS display API (Tailscale or local network IP)
# Default: http://100.64.0.0:8001 (Tailscale IP)
tars_url = http://100.64.0.0:8001
