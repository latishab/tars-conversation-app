[LLM]
# Available models: Any DeepInfra-supported model
# Examples: openai/gpt-oss-20b, meta-llama/Llama-3.3-70B-Instruct-Turbo, meta-llama/Llama-3.2-3B-Instruct
model = openai/gpt-oss-20b

# Gating model for intervention decisions (smaller/faster model recommended)
gating_model = meta-llama/Llama-3.2-3B-Instruct

[STT]
# Available providers: speechmatics, deepgram, deepgram-flux
# - speechmatics: Speechmatics with SMART_TURN detection
# - deepgram: Deepgram Nova-2 with endpoint detection
# - deepgram-flux: Deepgram Flux with built-in turn detection (recommended)
provider = deepgram-flux

[TTS]
# Available providers: elevenlabs, qwen3
provider = qwen3

# Qwen3-TTS Configuration (only used if provider = qwen3)
# Available models: Qwen/Qwen3-TTS-12Hz-0.6B-Base, Qwen/Qwen3-TTS-12Hz-1.7B-Base
qwen3_model = Qwen/Qwen3-TTS-12Hz-0.6B-Base
# Available devices: mps (Mac), cuda (NVIDIA), cpu
qwen3_device = mps
# Reference audio file for voice cloning (relative to project root)
qwen3_ref_audio = assets/audio/tars-clean-compressed.mp3

[Emotional]
# Enable real-time emotional state monitoring via video
enabled = true
# How often to sample video frames (in seconds)
sampling_interval = 3.0
# How many consecutive negative states before intervention
intervention_threshold = 2

[Connection]
# Transport mode: "robot" (aiortc WebRTC to RPi) or "browser" (SmallWebRTC for browser)
mode = robot
# Raspberry Pi WebRTC server URL (Tailscale or local network IP)
rpi_url = http://100.115.193.41:8001
# Auto-connect to RPi on startup (only for robot mode)
auto_connect = true
# Delay between reconnection attempts (seconds)
reconnect_delay = 5
# Maximum reconnection attempts (0 = infinite)
max_reconnect_attempts = 0

[Display]
# Enable TARS Raspberry Pi display integration (HTTP commands)
enabled = true
# URL of TARS display API (Tailscale or local network IP)
tars_url = http://100.115.193.41:8001
