# TARS Omni - Real-time Voice AI

Real-time voice AI with transcription, vision, and intelligent conversation using Speechmatics, Qwen3-TTS (or ElevenLabs), Qwen LLM, and Moondream.

## Features

- ğŸ¤ **Real-time Transcription** - Speechmatics with speaker diarization
- ğŸ”Š **Dual TTS** - Qwen3-TTS (local, free) or ElevenLabs (cloud)
- ğŸ¤– **LLM** - Qwen via DeepInfra
- ğŸ‘ï¸ **Vision** - Moondream image analysis
- ğŸ¯ **Smart Turn Detection** - VAD prevents interruptions
- ğŸš¦ **Gating Layer** - AI decides when to respond
- ğŸŒ **WebRTC** - Peer-to-peer audio/video
- ğŸ§  **Memory** - Optional Mem0 long-term memory
- ğŸ™ï¸ **Voice Cloning** - 3 seconds of audio with Qwen3-TTS
- ğŸ˜Š **Emotional Monitoring** - Real-time detection of confusion/hesitation/frustration

## Quick Start

### 1. Install Dependencies

```bash
# Python
pip install -r requirements.txt

# Node.js
npm install
```

### 2. Configure

```bash
cp env.example .env.local
# Edit .env.local with your API keys
```

Required keys:
- `SPEECHMATICS_API_KEY`
- `DEEPINFRA_API_KEY`
- `TTS_PROVIDER=qwen3` (or `elevenlabs`)

Optional:
- `ELEVENLABS_API_KEY` (if using cloud TTS)
- `MEM0_API_KEY` (for memory)

### 3. Run

```bash
# Terminal 1: Backend
npm run dev:backend

# Terminal 2: Frontend
npm run dev
```

Open http://localhost:3000

## Project Structure

```
tars-omni/
â”œâ”€â”€ app/                    # Next.js frontend
â”‚   â”œâ”€â”€ api/               # API routes
â”‚   â”œâ”€â”€ components/        # React components
â”‚   â””â”€â”€ page.tsx           # Main UI
â”‚
â”œâ”€â”€ pipecat_service.py     # FastAPI server
â”œâ”€â”€ bot.py                 # Pipeline orchestration
â”œâ”€â”€ loggers.py             # Monitoring processors
â”‚
â”œâ”€â”€ config/                # Environment config
â”œâ”€â”€ character/             # TARS personality
â”œâ”€â”€ processors/            # Frame processors
â”œâ”€â”€ services/              # AI services (TTS/STT/LLM)
â”œâ”€â”€ modules/               # LLM tools/functions
â”œâ”€â”€ memory/                # Mem0 integration
â””â”€â”€ scripts/               # Utilities
```

## Code Organization

| Type | Location | Purpose |
|------|----------|---------|
| **AI Service** | `services/` | TTS/STT/LLM/Vision integrations |
| **Processor** | `processors/` | Frame processing/filtering |
| **Logger** | `loggers.py` | Monitoring/debugging |
| **LLM Tool** | `modules/` | Functions the LLM can call |
| **Config** | `config/` | Environment variables |
| **Frontend** | `app/` | Next.js React app |

**Key Distinctions**:
- `services/` = AI engines (TTS, STT, LLM)
- `modules/` = LLM-callable functions (backend Python)
- `lib/` = Frontend utilities (TypeScript)
- `processors/` = Data processing
- `loggers.py` = Monitoring/observability

## TTS Configuration

### Qwen3-TTS (Default - Local & Free)

Best for Apple Silicon Macs. Voice cloning with `tars-clean-compressed.mp3`.

**M4 24GB Performance**:
- First load: ~15-20s
- Generation: 2.5-3x real-time
- Memory: ~2-3GB

```env
TTS_PROVIDER=qwen3
QWEN3_TTS_MODEL=Qwen/Qwen3-TTS-12Hz-0.6B-Base
QWEN3_TTS_DEVICE=mps
QWEN3_TTS_REF_AUDIO=tars-clean-compressed.mp3
```

### ElevenLabs (Cloud)

Better quality, requires API key and credits.

```env
TTS_PROVIDER=elevenlabs
ELEVENLABS_API_KEY=your_key
```

## How It Works

1. **Audio/Video Input** â†’ Browser captures via WebRTC
2. **Emotional Monitor** â†’ Analyzes video for confusion/hesitation (every 3s)
3. **VAD** â†’ Detects when user stops speaking
4. **STT** â†’ Speechmatics transcribes with speaker labels
5. **Gating** â†’ AI decides if TARS should respond
6. **LLM** â†’ Qwen processes and generates response
7. **Vision** â†’ Moondream analyzes images when requested
8. **TTS** â†’ Qwen3-TTS or ElevenLabs synthesizes speech
9. **Audio Output** â†’ Streamed back via WebRTC

## Tech Stack

**Frontend**: Next.js 16, React 19, Tailwind, shadcn/ui, WebRTC
**Backend**: Python 3.12, FastAPI, Pipecat.ai, PyTorch
**AI**: Speechmatics, Qwen3-TTS/ElevenLabs, Qwen LLM, Moondream

## Development

### Testing

```bash
python test_qwen_tts.py          # Qwen3-TTS standalone test
python test_qwen_pipecat.py      # Qwen3-TTS Pipecat integration
python test_emotional_monitor.py # Emotional monitoring test
```

### Switching TTS Providers

Edit `.env.local`:
```env
TTS_PROVIDER=elevenlabs  # or qwen3
```

### Voice Cloning

Place audio file in root, update `.env.local`:
```env
QWEN3_TTS_REF_AUDIO=your-voice.mp3
```

### Emotional Monitoring

TARS continuously analyzes your video feed for emotional cues and offers help proactively.

**Detects**:
- ğŸ˜• Confusion (puzzled expression, furrowed brow)
- ğŸ¤” Hesitation (pauses, uncertain gestures)
- ğŸ˜¤ Frustration (tense posture, agitated movements)

**Configuration**:
```env
EMOTIONAL_MONITORING_ENABLED=true       # Enable/disable
EMOTIONAL_SAMPLING_INTERVAL=3.0        # Analysis frequency (seconds)
EMOTIONAL_INTERVENTION_THRESHOLD=2     # Consecutive states before help
```

**How it works**:
1. Samples video frames every 3 seconds
2. Moondream analyzes emotional/cognitive state
3. Detects patterns indicating difficulty
4. After 2 consecutive negative states, TARS offers help

**Disable**: Set `EMOTIONAL_MONITORING_ENABLED=false`

## API Endpoints

**Backend (Port 7860)**:
- `POST /api/offer` - WebRTC offer
- `PATCH /api/offer` - ICE candidates
- `GET /api/status` - Health check

**Frontend (Port 3000)**:
- `/` - Main UI
- Proxies to backend

## License

MIT
